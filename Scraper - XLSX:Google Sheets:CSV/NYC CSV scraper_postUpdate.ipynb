{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData = pd.DataFrame(columns=['Date','RACE_GROUP','CASE_RATE_ADJ','HOSPITALIZED_RATE_ADJ','DEATH_RATE_ADJ','CASE_COUNT','HOSPITALIZED_COUNT','DEATH_COUNT'])\n",
    "\n",
    "confirmedProbableData = pd.DataFrame(columns = ['Date','RACE_GROUP','CONFIRMED_DEATH','PROBABLE_DEATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(url):\n",
    "    # this function will crawl through all of the next pages of the commit history for the by-race csv, and collect the link for \n",
    "    # each page\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    nextPage = bs.find(class_=\"btn btn-outline BtnGroup-item\",text='Older')\n",
    "    if nextPage.has_attr('href'):\n",
    "        link = nextPage['href']\n",
    "        return [url] + crawler(link)\n",
    "    else:\n",
    "        return [url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterLinks = crawler('https://github.com/nychealth/coronavirus-data/commits/master/totals/by-race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DUPLICATE COMMITS FOUND AT: https://github.com/nychealth/coronavirus-data/commits/master/totals/by-race.csv\n8 7\n"
     ]
    }
   ],
   "source": [
    "masterDays = list()\n",
    "masterCommits = list()\n",
    "\n",
    "diff = 0\n",
    "for masterLink in masterLinks:\n",
    "    # iterate through all of the links found with the crawler function\n",
    "    global masterDays\n",
    "    global masterCommits\n",
    "    html = urlopen(masterLink)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    masterDays = masterDays + bs.find_all(class_=\"f5 text-normal\")\n",
    "    # collect the commit upload dates in a global list called days\n",
    "\n",
    "    masterCommits = masterCommits + bs.find_all('clipboard-copy', class_=\"btn btn-outline BtnGroup-item\")\n",
    "    # collects all of the commit addresses/values into a global list called commits by collecting the \n",
    "    # commits off of the specific file page, instead of the master page for the entire NYC dataset, we ensure that each \n",
    "    # commit actually updated the data we wanted (i.e. doesn't correspond to a different file or README).\n",
    "    # Additionally, these commit addresses can be used for both the probable-confirmed file, as well as the master by \n",
    "    # race files.\n",
    "\n",
    "    if len(masterDays) != len(masterCommits) and abs(len(masterDays)-len(masterCommits)) != diff:\n",
    "        diff =  abs(len(masterDays)-len(masterCommits))\n",
    "        print('DUPLICATE COMMITS FOUND AT: ' + masterLink)\n",
    "    elif len(masterDays) == len(masterCommits) and diff!=0:\n",
    "        print('Anomaly at: ' + masterLink)\n",
    "        # collects the masterLink on which a specific date had multiple uploads, which lead to unequal lengths of each \n",
    "        # list\n",
    "print(len(masterCommits), len(masterDays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean each list of unnecessary scraped attributes/values\n",
    "masterDays = [y.text for y in masterDays]\n",
    "\n",
    "masterCommits = [x['value'] for x in masterCommits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above loop always raises duplicate error at \n",
    "# https://github.com/nychealth/coronavirus-data/commits/master/totals/by-race.csv, so I manually \n",
    "# looked at that page to find which date had multiple commits. It is 11/9, on which for some reason \n",
    "# two CSV's were uploaded. I manually found the commits for this date, so I can search through the \n",
    "# commit list and find their indexes, and add an additional date to correspond to their indexes in the \n",
    "# masterDays list.\n",
    "\n",
    "i = masterCommits.index('b671e0ed09b458f148110ad0d71479cf62580ea4')\n",
    "j = masterCommits.index('700a357f272b309fd8841b5e7dc4f4a53bb116ff')\n",
    "\n",
    "if (i >= len(masterDays)):\n",
    "    masterDays.append('Commits on Nov 9, 2020')\n",
    "elif (j >= len(masterDays)):\n",
    "    masterDays.append('Commits on Nov 9, 2020')\n",
    "elif (masterDays[i] != 'Commits on Nov 9, 2020'):\n",
    "    masterDays.insert(i, 'Commits on Nov 9, 2020')\n",
    "elif (masterDays[j] != 'Commits on Nov 9, 2020'):\n",
    "    masterDays.insert(j, 'Commits on Nov 9, 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "def commitChecker(commit,url):\n",
    "    # affirms that each commit actually goes to an existing csv. In the case where a given day had multiple commits \n",
    "    # for the same file, only one of those commit addresses would work, and so this function cleans our commits of \n",
    "    # any non functional addresses\n",
    "    url = url.split('[split]')\n",
    "    try:\n",
    "        html = urlopen(url[0] + str(commit) + url[1])\n",
    "    except HTTPError as e:\n",
    "        index = masterCommits.index(commit)\n",
    "        print(e)\n",
    "        print(\"bad commit: \" + commit + ' at ' + str(index))\n",
    "        print(str(masterDays[index]) + ' ' + commit)\n",
    "        masterDays.remove(masterDays[index])\n",
    "        # masterCommits.remove(commit)\n",
    "        print('removed from lists')\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "testing: len(commits) == len(days)\nexpected result: True\nactual result: True\nmatches expected? True\n8 8\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/[split]/totals/by-race.csv'\n",
    "\n",
    "masterCommits = [c for c in masterCommits if commitChecker(c,url)]\n",
    "\n",
    "# applies commitChecker function to commits\n",
    "\n",
    "# mini test to make sure that the lists for commits and days are now of equal length\n",
    "print(\"testing: len(commits) == len(days)\")\n",
    "result = len(masterCommits) == len(masterDays)\n",
    "print(\"expected result: True\")\n",
    "print(\"actual result: \" + str(result))\n",
    "print(\"matches expected? \" + str(result == True))\n",
    "print(len(masterCommits), len(masterDays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for day,commit in zip(masterDays,masterCommits):\n",
    "    # go through the days and commit values and create proper URLs based on the commit and the file name to create \n",
    "    # daily dataframes, which we append to a cumulative dataframe, in this instance for the by-race dataset.\n",
    "    \n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/nychealth/coronavirus-data/' + commit + '/totals/by-race.csv')\n",
    "    \n",
    "    df.insert(0, 'Date', day)\n",
    "    \n",
    "    total = {'Date':day, 'RACE_GROUP':'Totals', 'CASE_RATE_ADJ':df['CASE_RATE_ADJ'].sum(),'HOSPITALIZED_RATE_ADJ': df['HOSPITALIZED_RATE_ADJ'].sum(), 'DEATH_RATE_ADJ':df['DEATH_RATE_ADJ'].sum(), 'CASE_COUNT': df['CASE_COUNT'].sum(), 'HOSPITALIZED_COUNT': df['HOSPITALIZED_COUNT'].sum(), 'DEATH_COUNT':df['DEATH_COUNT'].sum()}\n",
    "    \n",
    "    # print(total)\n",
    "    df = df.append(total, ignore_index=True)\n",
    "    masterData = masterData.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the scraped master by-race dataFrame to a csv\n",
    "with open('./masterDataUpdate.csv', \"w\", newline='') as f:    \n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "masterData.to_csv ('./masterDataUpdate.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmedProbableData = pd.DataFrame(columns = ['Date','RACE_GROUP','CONFIRMED_DEATH','PROBABLE_DEATH'])\n",
    "probUrl = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/[split]/totals/probable-confirmed-by-race.csv'\n",
    "commitsProb = [c for c in masterCommits if commitChecker(c,probUrl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day,commit in zip(masterDays,commitsProb):\n",
    "    fd = pd.read_csv('https://raw.githubusercontent.com/nychealth/coronavirus-data/' + commit + '/totals/probable-confirmed-by-race.csv')\n",
    "    fd.insert(0,'Date',day)\n",
    "    totals = {'Date':day,'RACE_GROUP':'Totals','CONFIRMED_DEATH':fd['CONFIRMED_DEATH'].sum(),'PROBABLE_DEATH':fd['PROBABLE_DEATH'].sum()}\n",
    "    fd = fd.append(totals, ignore_index=True)\n",
    "    confirmedProbableData = confirmedProbableData.append(fd,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./confirmedProbableDeathDataUpdate.csv', \"w\", newline='') as f:    \n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "confirmedProbableData.to_csv ('./confirmedProbableDeathDataUpdate.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean date columns for github csv upload\n",
    "\n",
    "file_date = masterData['Date'][0].replace(',','').split()\n",
    "file_date = str(file_date[2]) + '_' + str(file_date[3])\n",
    "\n",
    "months = dict([('Jan','01'), ('Feb','02'),('Mar','03'),('Apr','04'),('May','05'),('Jun','06'),('Jul','07'),('Aug','08'),('Sep','09'),('Oct','10'),('Nov','11'),('Dec','12')])\n",
    "# define dates dictionary with the keys being the 3 letter abbreviations used in the commit dates on github, and \n",
    "# the values being the appropriate 2 digit numbers for each date\n",
    "\n",
    "def convert_date(commit):\n",
    "    vals = commit.split(' ')[2:]\n",
    "    month = months[vals[0]]\n",
    "    day = vals[1].replace(',','')\n",
    "    if len(day) == 1:\n",
    "        day = '0' + day\n",
    "    year = vals[2]\n",
    "    result = year + '-' + month + '-' + day\n",
    "    return date.fromisoformat(result)\n",
    "\n",
    "masterData['Date'] = masterData['Date'].apply(convert_date)\n",
    "\n",
    "confirmedProbableData['Date'] = confirmedProbableData['Date'].apply(convert_date)\n",
    "\n",
    "with open('./' + file_date + ' - master data by race - NYC.csv', \"w\", newline='') as out:    \n",
    "    writer = csv.writer(out, delimiter=',')\n",
    "masterData.to_csv ('./' + file_date + ' - master data by race - NYC.csv', index = False, header=True)\n",
    "\n",
    "with open('./' + file_date + ' - confirmed probable deaths by race - NYC.csv', \"w\", newline='') as outfile:    \n",
    "    writer = csv.writer(outfile, delimiter=',')\n",
    "confirmedProbableData.to_csv ('./' + file_date + ' - confirmed probable deaths by race - NYC.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}