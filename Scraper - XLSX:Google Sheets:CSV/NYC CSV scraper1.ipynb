{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "def ChromeDriver(url):\n",
    "    chrome_path= \"/usr/local/bin/chromedriver\"\n",
    "    # These are preliminary settings on our chromedriver, we want to make sure it's a maximized browser, disable any\n",
    "    # infobars and extensions.\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\");\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # This line of code now actually declares the driver, it has two arguments 'executable_path' where we feed in our\n",
    "    # directory path for the chromedriver and the 'options' which are the chromedriver settings from above.\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n",
    "    driver.get(url)\n",
    "    # We declare the url and execute the driver to fetch the URL\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success! to Commits on Oct 2, 2020\n"
     ]
    }
   ],
   "source": [
    "driver = ChromeDriver('https://github.com/nychealth/coronavirus-data/commits/master/deaths/probable-confirmed-by-race.csv')\n",
    "# opens main blame page\n",
    "html = driver.page_source\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = bs.find_all(class_=\"text-mono f6 btn btn-outline BtnGroup-item\")\n",
    "# collects all of the commit links for each daily update\n",
    "dates = bs.find_all(class_=\"f5 text-normal\")\n",
    "# collects all of the corresponding commit dates\n",
    "driver.quit()\n",
    "\n",
    "totalData = pd.DataFrame(columns = ['Date','RACE_GROUP','CONFIRMED_DEATH','PROBABLE_DEATH'])\n",
    "# declares new dataframe with necessary columns\n",
    "\n",
    "for date,link in zip(dates,links):\n",
    "    link = 'https://github.com/' + link['href'].split('#')[0] + '/deaths/probable-confirmed-by-race.csv'\n",
    "    link = link.replace('commit', 'raw')\n",
    "    # Modify scraped link to get corresponding raw data link\n",
    "\n",
    "    df = pd.read_csv(link)\n",
    "    # pass raw csv link directly into pandas and create a separate dataframe for that days update\n",
    "\n",
    "    df.insert(0, 'Date', date.text)\n",
    "    # create new 'Date' column in that days corresponding dataframe and fill it with the text from the scraped       # attributes\n",
    "    \n",
    "    totalData = totalData.append(df)\n",
    "    # append new dataframe to cumulative total dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success! to Commits on Aug 26, 2020\n"
     ]
    }
   ],
   "source": [
    "driver = ChromeDriver('https://github.com/nychealth/coronavirus-data/commits/master?after=bb150d5ed4b9e00ddca371ea12e198c6c03f8b18+34&branch=master&path%5B%5D=deaths&path%5B%5D=probable-confirmed-by-race.csv')\n",
    "# opens the next page of blame (currently working on a way to collect links to next pages automatically, if my \n",
    "# computer can handle that!)\n",
    "\n",
    "html = driver.page_source\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = bs.find_all(class_=\"text-mono f6 btn btn-outline BtnGroup-item\")\n",
    "# collects all of the commit links for each daily update\n",
    "dates = bs.find_all(class_=\"f5 text-normal\")\n",
    "# collects all of the corresponding commit dates\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "for date,link in zip(dates,links):\n",
    "    link = 'https://github.com/' + link['href'].split('#')[0] + '/deaths/probable-confirmed-by-race.csv'\n",
    "    link = link.replace('commit', 'raw')\n",
    "    # Modify scraped link to get corresponding raw data link\n",
    "\n",
    "    df = pd.read_csv(link)\n",
    "    # pass raw csv link directly into pandas and create a separate dataframe for that days update\n",
    "\n",
    "    df.insert(0, 'Date', date.text)\n",
    "    # create new 'Date' column in that days corresponding dataframe and fill it with the text from the scraped       # attributes\n",
    "    totalData = totalData.append(df)\n",
    "    # append new dataframe to cumulative total dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success! to Commits on Jul 22, 2020\n"
     ]
    }
   ],
   "source": [
    "driver = ChromeDriver('https://github.com/nychealth/coronavirus-data/commits/master?after=bb150d5ed4b9e00ddca371ea12e198c6c03f8b18+69&branch=master&path%5B%5D=deaths&path%5B%5D=probable-confirmed-by-race.csv')\n",
    "html = driver.page_source\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = bs.find_all(class_=\"text-mono f6 btn btn-outline BtnGroup-item\")\n",
    "# collects all of the commit links for each daily update\n",
    "dates = bs.find_all(class_=\"f5 text-normal\")\n",
    "# collects all of the corresponding commit dates\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "for date,link in zip(dates,links):\n",
    "    link = 'https://github.com/' + link['href'].split('#')[0] + '/deaths/probable-confirmed-by-race.csv'\n",
    "    link = link.replace('commit', 'raw')\n",
    "    # Modify scraped link to get corresponding raw data link\n",
    "\n",
    "    df = pd.read_csv(link)\n",
    "    # pass raw csv link directly into pandas and create a separate dataframe for that days update\n",
    "\n",
    "    df.insert(0, 'Date', date.text)\n",
    "    # create new 'Date' column in that days corresponding dataframe and fill it with the text from the scraped       # attributes\n",
    "    totalData = totalData.append(df)\n",
    "    # append new dataframe to cumulative total dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success! to Commits on Jun 16, 2020\n"
     ]
    }
   ],
   "source": [
    "driver = ChromeDriver('https://github.com/nychealth/coronavirus-data/commits/master?after=bb150d5ed4b9e00ddca371ea12e198c6c03f8b18+104&branch=master&path%5B%5D=deaths&path%5B%5D=probable-confirmed-by-race.csv')\n",
    "html = driver.page_source\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = bs.find_all(class_=\"text-mono f6 btn btn-outline BtnGroup-item\")\n",
    "# collects all of the commit links for each daily update\n",
    "dates = bs.find_all(class_=\"f5 text-normal\")\n",
    "# collects all of the corresponding commit dates\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "for date,link in zip(dates,links):\n",
    "    link = 'https://github.com/' + link['href'].split('#')[0] + '/deaths/probable-confirmed-by-race.csv'\n",
    "    link = link.replace('commit', 'raw')\n",
    "    # Modify scraped link to get corresponding raw data link\n",
    "\n",
    "    df = pd.read_csv(link)\n",
    "    # pass raw csv link directly into pandas and create a separate dataframe for that days update\n",
    "\n",
    "    df.insert(0, 'Date', date.text)\n",
    "    # create new 'Date' column in that days corresponding dataframe and fill it with the text from the scraped       # attributes\n",
    "    totalData = totalData.append(df)\n",
    "    # append new dataframe to cumulative total dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success! to Commits on May 18, 2020\n"
     ]
    }
   ],
   "source": [
    "driver = ChromeDriver('https://github.com/nychealth/coronavirus-data/commits/master?after=bb150d5ed4b9e00ddca371ea12e198c6c03f8b18+139&branch=master&path%5B%5D=deaths&path%5B%5D=probable-confirmed-by-race.csv')\n",
    "html = driver.page_source\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = bs.find_all(class_=\"text-mono f6 btn btn-outline BtnGroup-item\")\n",
    "# collects all of the commit links for each daily update\n",
    "dates = bs.find_all(class_=\"f5 text-normal\")\n",
    "# collects all of the corresponding commit dates\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "for date,link in zip(dates,links):\n",
    "    link = 'https://github.com/' + link['href'].split('#')[0] + '/deaths/probable-confirmed-by-race.csv'\n",
    "    link = link.replace('commit', 'raw')\n",
    "    # Modify scraped link to get corresponding raw data link\n",
    "\n",
    "    df = pd.read_csv(link)\n",
    "    # pass raw csv link directly into pandas and create a separate dataframe for that days update\n",
    "\n",
    "    df.insert(0, 'Date', date.text)\n",
    "    # create new 'Date' column in that days corresponding dataframe and fill it with the text from the scraped       # attributes\n",
    "    totalData = totalData.append(df)\n",
    "    # append new dataframe to cumulative total dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dict([('Jan','01'), ('Feb','02'),('Mar','03'),('Apr','04'),('May','05'),('Jun','06'),('Jul','07'),('Aug','08'),('Sep','09'),('Oct','10'),('Nov','11'),('Dec','12')])\n",
    "# define dates dictionary with the keys being the 3 letter abbreviations used in the commit dates on github, and \n",
    "# the values being the appropriate 2 digit numbers for each date\n",
    "\n",
    "def convert_date(commit):\n",
    "    date = commit.split(' ')[2:]\n",
    "    month = dates[date[0]]\n",
    "    day = date[1].replace(',','')\n",
    "    if len(day) == 1:\n",
    "        day = '0' + day\n",
    "    year = date[2]\n",
    "    result = year + '-' + month + '-' + day\n",
    "    return result\n",
    "\n",
    "totalData['Date'] = totalData['Date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Date              RACE_GROUP CONFIRMED_DEATH PROBABLE_DEATH  \\\n",
       "0   2020-11-05         Hispanic/Latino            5987           1264   \n",
       "1   2020-11-05  Black/African-American            5364           1435   \n",
       "2   2020-11-05                   White            4931           1201   \n",
       "3   2020-11-05  Asian/Pacific-Islander            1440            413   \n",
       "4   2020-11-05           Other/Unknown            1662            334   \n",
       "..         ...                     ...             ...            ...   \n",
       "1   2020-05-18  Black/African-American            4409           1347   \n",
       "2   2020-05-18                   White            4039           1101   \n",
       "3   2020-05-18  Asian/Pacific-Islander            1220            389   \n",
       "4   2020-05-18           Other/Unknown            1569            384   \n",
       "5   2020-05-18            Data pending               0            449   \n",
       "\n",
       "   CONF + PROB DEATHS  \n",
       "0                7251  \n",
       "1                6799  \n",
       "2                6132  \n",
       "3                1853  \n",
       "4                1996  \n",
       "..                ...  \n",
       "1                5756  \n",
       "2                5140  \n",
       "3                1609  \n",
       "4                1953  \n",
       "5                 449  \n",
       "\n",
       "[1008 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>RACE_GROUP</th>\n      <th>CONFIRMED_DEATH</th>\n      <th>PROBABLE_DEATH</th>\n      <th>CONF + PROB DEATHS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-11-05</td>\n      <td>Hispanic/Latino</td>\n      <td>5987</td>\n      <td>1264</td>\n      <td>7251</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-11-05</td>\n      <td>Black/African-American</td>\n      <td>5364</td>\n      <td>1435</td>\n      <td>6799</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-11-05</td>\n      <td>White</td>\n      <td>4931</td>\n      <td>1201</td>\n      <td>6132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-11-05</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>1440</td>\n      <td>413</td>\n      <td>1853</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-11-05</td>\n      <td>Other/Unknown</td>\n      <td>1662</td>\n      <td>334</td>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-05-18</td>\n      <td>Black/African-American</td>\n      <td>4409</td>\n      <td>1347</td>\n      <td>5756</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-05-18</td>\n      <td>White</td>\n      <td>4039</td>\n      <td>1101</td>\n      <td>5140</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-05-18</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>1220</td>\n      <td>389</td>\n      <td>1609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-05-18</td>\n      <td>Other/Unknown</td>\n      <td>1569</td>\n      <td>384</td>\n      <td>1953</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2020-05-18</td>\n      <td>Data pending</td>\n      <td>0</td>\n      <td>449</td>\n      <td>449</td>\n    </tr>\n  </tbody>\n</table>\n<p>1008 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "totalData['CONF + PROB DEATHS'] = totalData['CONFIRMED_DEATH']+ totalData['PROBABLE_DEATH']\n",
    "# create new\n",
    "\n",
    "totalData\n",
    "\n",
    "with open('./totalData.csv', \"w\", newline='') as f:    \n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "totalData.to_csv ('./totalData.csv', index = False, header=True)"
   ]
  }
 ]
}