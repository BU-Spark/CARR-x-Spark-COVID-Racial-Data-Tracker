{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "def ChromeDriver(url):\n",
    "    chrome_path= \"/usr/local/bin/chromedriver\"\n",
    "    # These are preliminary settings on our chromedriver, we want to make sure it's a maximized browser, disable any\n",
    "    # infobars and extensions.\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\");\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # This line of code now actually declares the driver, it has two arguments 'executable_path' where we feed in our\n",
    "    # directory path for the chromedriver and the 'options' which are the chromedriver settings from above.\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n",
    "    driver.get(url)\n",
    "    # We declare the url and execute the driver to fetch the URL\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData = pd.read_csv('masterData.csv')\n",
    "masterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://github.com/nychealth/coronavirus-data/commits/master?after=b671e0ed09b458f148110ad0d71479cf62580ea4+34&branch=master&path%5B%5D=by-race.csv\n",
      "\n",
      "https://github.com/nychealth/coronavirus-data/commits/master?after=b671e0ed09b458f148110ad0d71479cf62580ea4+69&branch=master&path%5B%5D=by-race.csv\n",
      "\n",
      "https://github.com/nychealth/coronavirus-data/commits/master?after=b671e0ed09b458f148110ad0d71479cf62580ea4+104&branch=master&path%5B%5D=by-race.csv\n",
      "\n",
      "https://github.com/nychealth/coronavirus-data/commits/master?after=b671e0ed09b458f148110ad0d71479cf62580ea4+139&branch=master&path%5B%5D=by-race.csv\n",
      "\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "masterLinks = ['https://github.com/nychealth/coronavirus-data/commits/master/by-race.csv']\n",
    "def crawler(url):\n",
    "    # this function will crawl through all of the next pages of the commit history for the by-race csv, and collect the link for each page\n",
    "    global masterLinks\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    nextPage = bs.find(class_=\"btn btn-outline BtnGroup-item\",text='Older')\n",
    "    if nextPage.has_attr('href') and nextPage['href'] not in masterLinks:\n",
    "        link = nextPage['href']\n",
    "        masterLinks = masterLinks + [link]\n",
    "        return crawler(link)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "crawler('https://github.com/nychealth/coronavirus-data/commits/master/by-race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FLAG: NUMBER OF COMMIT ADDRESSES IS NOT EQUAL TO NUMBER OF DATES\n\nDISCREPANCY FOUND AT: https://github.com/nychealth/coronavirus-data/commits/master?after=b671e0ed09b458f148110ad0d71479cf62580ea4+34&branch=master&path%5B%5D=by-race.csv\n"
     ]
    }
   ],
   "source": [
    "days = list()\n",
    "commits = list()\n",
    "\n",
    "catch = ''\n",
    "for masterLink in masterLinks:\n",
    "    # iterate through all of the links found with the crawler function\n",
    "    global days\n",
    "    global commits\n",
    "    html = urlopen(masterLink)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    days = days + bs.find_all(class_=\"f5 text-normal\")\n",
    "    # collect the commit upload dates in a global list called days\n",
    "\n",
    "    commits = commits + bs.find_all('clipboard-copy', class_=\"btn btn-outline BtnGroup-item\")\n",
    "    # collects all of the commit addresses/values into a global list called commits by collecting the \n",
    "    # commits off of the specific file page, instead of the master page for the entire NYC dataset, we ensure that each \n",
    "    # commit actually updated the data we wanted (i.e. doesn't correspond to a different file or README).\n",
    "    # Additionally, these commit addresses can be used for both the probable-confirmed file, as well as the master by race      # files.\n",
    "\n",
    "    if len(days) != len(commits) and catch == '':\n",
    "        # collects the masterLink on which a specific date had multiple uploads, which lead to unequal lengths of each list\n",
    "        catch = masterLink\n",
    "\n",
    "if len(commits) != len(days):\n",
    "    print('DUPLICATE COMMITS FOUND AT: ' + catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "def commitChecker(commit):\n",
    "    # affirms that each commit actually goes to an existing csv. In the case where a given day had multiple commits \n",
    "    # for the same file, only one of those commit addresses would work, and so this function cleans our commits of \n",
    "    # any non functional addresses\n",
    "    try:\n",
    "        html = urlopen('https://raw.githubusercontent.com/nychealth/coronavirus-data/' + commit + '/by-race.csv')\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        print(\"bad commit: \" + commit)\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HTTP Error 404: Not Found\n",
      "bad commit: 700a357f272b309fd8841b5e7dc4f4a53bb116ff\n",
      "testing: len(commits) == len(days)\n",
      "expected result: True\n",
      "actual result: True\n",
      "matches expected? True\n"
     ]
    }
   ],
   "source": [
    "# clean each list of unnecessary scraped attributes/values\n",
    "days = [y.text for y in days]\n",
    "\n",
    "commits = [x['value'] for x in commits]\n",
    "\n",
    "commits = [c for c in commits if commitChecker(c)]\n",
    "# applies commitChecker function to commits\n",
    "\n",
    "# mini test to make sure that the lists for commits and days are now of equal length\n",
    "print(\"testing: len(commits) == len(days)\")\n",
    "result = len(commits) == len(days)\n",
    "print(\"expected result: True\")\n",
    "print(\"actual result: \" + str(result))\n",
    "print(\"matches expected? \" + str(result == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day,commit in zip(days,commits):\n",
    "    # go through the days and commit values and create proper URLs based on the commit and the file name to create \n",
    "    # daily dataframes, which we append to a cumulative dataframe, in this instance for the by-race dataset.\n",
    "    print(str(day) + ': ' + str(commit))\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/nychealth/coronavirus-data/' + commit + '/by-race.csv')\n",
    "    df.insert(0, 'Date', day)\n",
    "    masterData = masterData.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     Date              RACE_GROUP  CASE_RATE_ADJ  \\\n",
       "0  Commits on Nov 9, 2020  Asian/Pacific-Islander         922.14   \n",
       "1  Commits on Nov 9, 2020  Black/African-American        1941.90   \n",
       "2  Commits on Nov 9, 2020         Hispanic/Latino        2199.41   \n",
       "3  Commits on Nov 9, 2020                   White        1426.05   \n",
       "4  Commits on Nov 8, 2020  Asian/Pacific-Islander         916.27   \n",
       "\n",
       "   HOSPITALIZED_RATE_ADJ  DEATH_RATE_ADJ CASE_COUNT HOSPITALIZED_COUNT  \\\n",
       "0                 293.56          111.99      12365               3981   \n",
       "1                 738.67          249.62      39619              15804   \n",
       "2                 743.55          268.58      53542              17478   \n",
       "3                 349.00          125.83      43397              12480   \n",
       "4                 293.10          111.86      12286               3974   \n",
       "\n",
       "  DEATH_COUNT  \n",
       "0        1441  \n",
       "1        5364  \n",
       "2        5995  \n",
       "3        4938  \n",
       "4        1439  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>RACE_GROUP</th>\n      <th>CASE_RATE_ADJ</th>\n      <th>HOSPITALIZED_RATE_ADJ</th>\n      <th>DEATH_RATE_ADJ</th>\n      <th>CASE_COUNT</th>\n      <th>HOSPITALIZED_COUNT</th>\n      <th>DEATH_COUNT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>922.14</td>\n      <td>293.56</td>\n      <td>111.99</td>\n      <td>12365</td>\n      <td>3981</td>\n      <td>1441</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Black/African-American</td>\n      <td>1941.90</td>\n      <td>738.67</td>\n      <td>249.62</td>\n      <td>39619</td>\n      <td>15804</td>\n      <td>5364</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Hispanic/Latino</td>\n      <td>2199.41</td>\n      <td>743.55</td>\n      <td>268.58</td>\n      <td>53542</td>\n      <td>17478</td>\n      <td>5995</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>White</td>\n      <td>1426.05</td>\n      <td>349.00</td>\n      <td>125.83</td>\n      <td>43397</td>\n      <td>12480</td>\n      <td>4938</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Commits on Nov 8, 2020</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>916.27</td>\n      <td>293.10</td>\n      <td>111.86</td>\n      <td>12286</td>\n      <td>3974</td>\n      <td>1439</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "masterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the scraped master by-race dataFrame to a csv\n",
    "with open('./masterData.csv', \"w\", newline='') as f:    \n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "masterData.to_csv ('./masterData.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmedProbableData = pd.read_csv('confirmedProbableDeathData.csv')\n",
    "# do the same thing as above for the confirmed-probable dataset. \n",
    "# We can use the same day/commit lists for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day,commit in zip(days,commits):\n",
    "    fd = pd.read_csv('https://raw.githubusercontent.com/nychealth/coronavirus-data/' + commit + '/deaths/probable-confirmed-by-race.csv')\n",
    "    fd.insert(0,'Date',day)\n",
    "    confirmedProbableData = confirmedProbableData.append(fd,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         Date              RACE_GROUP CONFIRMED_DEATH  \\\n",
       "0      Commits on Nov 9, 2020         Hispanic/Latino            5995   \n",
       "1      Commits on Nov 9, 2020  Black/African-American            5364   \n",
       "2      Commits on Nov 9, 2020                   White            4938   \n",
       "3      Commits on Nov 9, 2020  Asian/Pacific-Islander            1441   \n",
       "4      Commits on Nov 9, 2020           Other/Unknown            1672   \n",
       "...                       ...                     ...             ...   \n",
       "1033  Commits on May 18, 2020  Black/African-American            4409   \n",
       "1034  Commits on May 18, 2020                   White            4039   \n",
       "1035  Commits on May 18, 2020  Asian/Pacific-Islander            1220   \n",
       "1036  Commits on May 18, 2020           Other/Unknown            1569   \n",
       "1037  Commits on May 18, 2020            Data pending               0   \n",
       "\n",
       "     PROBABLE_DEATH  \n",
       "0              1263  \n",
       "1              1436  \n",
       "2              1203  \n",
       "3               413  \n",
       "4               334  \n",
       "...             ...  \n",
       "1033           1347  \n",
       "1034           1101  \n",
       "1035            389  \n",
       "1036            384  \n",
       "1037            449  \n",
       "\n",
       "[1038 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>RACE_GROUP</th>\n      <th>CONFIRMED_DEATH</th>\n      <th>PROBABLE_DEATH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Hispanic/Latino</td>\n      <td>5995</td>\n      <td>1263</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Black/African-American</td>\n      <td>5364</td>\n      <td>1436</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>White</td>\n      <td>4938</td>\n      <td>1203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>1441</td>\n      <td>413</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Commits on Nov 9, 2020</td>\n      <td>Other/Unknown</td>\n      <td>1672</td>\n      <td>334</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1033</th>\n      <td>Commits on May 18, 2020</td>\n      <td>Black/African-American</td>\n      <td>4409</td>\n      <td>1347</td>\n    </tr>\n    <tr>\n      <th>1034</th>\n      <td>Commits on May 18, 2020</td>\n      <td>White</td>\n      <td>4039</td>\n      <td>1101</td>\n    </tr>\n    <tr>\n      <th>1035</th>\n      <td>Commits on May 18, 2020</td>\n      <td>Asian/Pacific-Islander</td>\n      <td>1220</td>\n      <td>389</td>\n    </tr>\n    <tr>\n      <th>1036</th>\n      <td>Commits on May 18, 2020</td>\n      <td>Other/Unknown</td>\n      <td>1569</td>\n      <td>384</td>\n    </tr>\n    <tr>\n      <th>1037</th>\n      <td>Commits on May 18, 2020</td>\n      <td>Data pending</td>\n      <td>0</td>\n      <td>449</td>\n    </tr>\n  </tbody>\n</table>\n<p>1038 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "confirmedProbableData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./confirmedProbableDeathData.csv', \"w\", newline='') as f:    \n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "masterData.to_csv ('./confirmedProbableDeathData.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}